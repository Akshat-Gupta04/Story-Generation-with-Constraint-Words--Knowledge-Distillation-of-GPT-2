{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9faab318-178f-4ba1-9213-afacf970d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.51.0\n",
      "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: datasets==2.20.0 in /usr/local/lib/python3.11/dist-packages (2.20.0)\n",
      "Requirement already satisfied: matplotlib==3.9.0 in /usr/local/lib/python3.11/dist-packages (3.9.0)\n",
      "Requirement already satisfied: tqdm==4.66.5 in /usr/local/lib/python3.11/dist-packages (4.66.5)\n",
      "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.0)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.51.0) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (20.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (2.3.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0) (3.12.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.9.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (2.9.0.post0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2) (2.8.0.dev20250319+cu128)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.0) (1.1.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.51.0) (2025.1.31)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.34.2) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch>=1.10.0->accelerate==0.34.2) (77.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0) (2025.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (2.1.5)\n",
      "Downloading transformers-4.51.0-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m256.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m288.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.45.0\n",
      "    Uninstalling transformers-4.45.0:\n",
      "      Successfully uninstalled transformers-4.45.0\n",
      "Successfully installed tokenizers-0.21.1 transformers-4.51.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages with updated versions\n",
    "!pip install transformers==4.51.0 datasets==2.20.0 matplotlib==3.9.0 tqdm==4.66.5 accelerate==0.34.2 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087cfd17-d53e-4889-9902-7e657aecfc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers Version: 4.51.0\n",
      "BF16 Supported: True\n",
      "Date and Time: 03:54 PM IST, Thursday, June 12, 2025\n",
      "Device: cuda\n",
      "Teacher Model: Qwen/Qwen3-1.7B\n",
      "Student Model: openai-community/gpt2\n",
      "Batch Size: 128, Epochs: 2, Max Length: 128\n",
      "Max New Tokens for Generation: 50\n",
      "Inference Subset Size: 20\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import transformers\n",
    "\n",
    "# Confirm transformers version\n",
    "print(f\"Transformers Version: {transformers.__version__}\")\n",
    "\n",
    "# Configuration\n",
    "teacher_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "student_model_name = \"openai-community/gpt2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128  # Can increase to 256 if memory allows (A100 SXM has 80GB VRAM)\n",
    "num_epochs = 2\n",
    "max_length = 128\n",
    "max_new_tokens = 50\n",
    "inference_subset_size = 20\n",
    "\n",
    "# Check if BF16 is supported (A100 SXM supports BF16 natively)\n",
    "bf16_supported = torch.cuda.is_bf16_supported()\n",
    "print(f\"BF16 Supported: {bf16_supported}\")\n",
    "\n",
    "print(f\"Date and Time: 03:54 PM IST, Thursday, June 12, 2025\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Teacher Model: {teacher_model_name}\")\n",
    "print(f\"Student Model: {student_model_name}\")\n",
    "print(f\"Batch Size: {batch_size}, Epochs: {num_epochs}, Max Length: {max_length}\")\n",
    "print(f\"Max New Tokens for Generation: {max_new_tokens}\")\n",
    "print(f\"Inference Subset Size: {inference_subset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c1280ca-4419-4e36-8844-d51801bbe669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 16207\n",
      "Validation Dataset Size: 1817\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"Ximing/ROCStories\")\n",
    "\n",
    "# Preprocess dataset for generation\n",
    "def preprocess_function(examples):\n",
    "    prompts = examples['prompt']\n",
    "    continuations = examples['continuation']\n",
    "    constraint_words = examples['constraint_words']\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    inputs = [f\"{prompt} {continuation}\" for prompt, continuation in zip(prompts, continuations)]\n",
    "    tokenized = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"prompts\": prompts,\n",
    "        \"continuations\": continuations,\n",
    "        \"constraint_words\": constraint_words\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "train_dataset = tokenized_dataset[\"train\"]\n",
    "eval_dataset = tokenized_dataset[\"validation\"]\n",
    "\n",
    "print(f\"Training Dataset Size: {len(train_dataset)}\")\n",
    "print(f\"Validation Dataset Size: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22267d6-c7c7-4e02-aa07-f4c54fed3026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33eb82d2993947a48193652251ff0975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing teacher outputs for training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precomputing Teacher Outputs: 100%|██████████| 127/127 [13:18<00:00,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of teacher_outputs_list: 16207\n",
      "Length of train_dataset: 16207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ae88b7fb7a4424913544c3386c78c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from train_dataset after mapping:\n",
      "Keys in sample: ['story_id', 'prompt', 'continuation', 'constraint_words', 'input_ids', 'attention_mask', 'prompts', 'continuations', 'teacher_outputs']\n",
      "teacher_outputs sample: On my way to work I stopped to get some coffee. I ordered a coffee, and the coffee was 30% less than the original price. The total cost was 35. What is the original price of the coffee?\n",
      "\n",
      "Let me think. So, the problem says that I ordered a coffee\n",
      "Models and tokenizers loaded successfully. Teacher outputs precomputed.\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_name).to(device)\n",
    "student_model = AutoModelForCausalLM.from_pretrained(student_model_name).to(device)\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n",
    "student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "student_tokenizer.padding_side = \"left\"\n",
    "teacher_tokenizer.padding_side = \"left\"\n",
    "\n",
    "teacher_model.eval()\n",
    "student_model.train()\n",
    "\n",
    "# Precompute teacher outputs for the training dataset\n",
    "print(\"Precomputing teacher outputs for training dataset...\")\n",
    "teacher_outputs_list = []\n",
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(train_dataset), batch_size), desc=\"Precomputing Teacher Outputs\"):\n",
    "        batch = train_dataset[i:i+batch_size]\n",
    "        prompts = batch[\"prompts\"]\n",
    "        teacher_inputs = teacher_tokenizer(\n",
    "            prompts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True\n",
    "        ).to(device)\n",
    "        \n",
    "        teacher_outputs = teacher_model.generate(\n",
    "            input_ids=teacher_inputs[\"input_ids\"],\n",
    "            attention_mask=teacher_inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # Removed early_stopping since num_beams=1 (greedy decoding)\n",
    "        )\n",
    "        \n",
    "        teacher_continuations = [teacher_tokenizer.decode(output, skip_special_tokens=True) for output in teacher_outputs]\n",
    "        teacher_outputs_list.extend(teacher_continuations)\n",
    "\n",
    "# Verify the length of teacher_outputs_list matches the dataset\n",
    "print(f\"Length of teacher_outputs_list: {len(teacher_outputs_list)}\")\n",
    "print(f\"Length of train_dataset: {len(train_dataset)}\")\n",
    "if len(teacher_outputs_list) != len(train_dataset):\n",
    "    raise ValueError(\"Mismatch between teacher_outputs_list and train_dataset lengths!\")\n",
    "\n",
    "# Add teacher outputs to the dataset\n",
    "def add_teacher_outputs(examples, idx):\n",
    "    # idx is a list of indices for the examples in this batch; use idx[0] as the starting index\n",
    "    start_idx = idx[0]\n",
    "    end_idx = idx[-1] + 1  # idx[-1] is the last index in the batch; add 1 to include it in the slice\n",
    "    examples[\"teacher_outputs\"] = teacher_outputs_list[start_idx:end_idx]\n",
    "    return examples\n",
    "\n",
    "# Apply the mapping with indices to ensure correct alignment\n",
    "train_dataset = train_dataset.map(add_teacher_outputs, with_indices=True, batched=True)\n",
    "\n",
    "# Debug: Check if teacher_outputs is added\n",
    "print(\"Sample from train_dataset after mapping:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Keys in sample: {list(sample.keys())}\")\n",
    "if \"teacher_outputs\" in sample:\n",
    "    print(f\"teacher_outputs sample: {sample['teacher_outputs']}\")\n",
    "else:\n",
    "    raise ValueError(\"teacher_outputs not found in dataset after mapping!\")\n",
    "\n",
    "print(\"Models and tokenizers loaded successfully. Teacher outputs precomputed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "972ec97b-a484-4aba-9bac-496de8e20544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom trainer defined.\n"
     ]
    }
   ],
   "source": [
    "# Custom Trainer for distillation\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, teacher_tokenizer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_tokenizer = teacher_tokenizer\n",
    "        self.train_losses = []\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        input_ids = inputs.pop(\"input_ids\").to(device)\n",
    "        attention_mask = inputs.pop(\"attention_mask\").to(device)\n",
    "        teacher_continuations = inputs.pop(\"teacher_outputs\")\n",
    "        \n",
    "        # Convert teacher outputs to student tokenizer's vocabulary\n",
    "        student_tokenizer.padding_side = \"left\"\n",
    "        student_labels = student_tokenizer(\n",
    "            teacher_continuations,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True\n",
    "        )[\"input_ids\"].to(device)\n",
    "        \n",
    "        # Student forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=student_labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "        self.train_losses.append(loss.item())\n",
    "        return loss\n",
    "\n",
    "print(\"Custom trainer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a9c978d-640b-4aa9-8537-b80ee5731cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='254' max='254' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [254/254 01:25, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.699700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed checkpoint: /workspace/distillation_results/checkpoint-254.tar.gz\n",
      "Compressed checkpoint: /workspace/distillation_results/checkpoint-127.tar.gz\n",
      "\n",
      "Training completed.\n",
      "Training Loss per Epoch: ['4.6367', '2.7681']\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define a custom data collator that handles teacher_outputs and excludes other string fields\n",
    "def custom_data_collator(features):\n",
    "    # Extract string fields that we don't want to tensorize\n",
    "    teacher_outputs = [f.pop(\"teacher_outputs\") for f in features]\n",
    "    prompts = [f.pop(\"prompts\") for f in features]  # Not needed for training\n",
    "    continuations = [f.pop(\"continuations\") for f in features]  # Not needed for training\n",
    "    constraint_words = [f.pop(\"constraint_words\") for f in features]  # Not needed for training\n",
    "    \n",
    "    # Use the default collator for the remaining fields (input_ids, attention_mask)\n",
    "    batch = default_data_collator(features)\n",
    "    \n",
    "    # Add teacher_outputs back to the batch as a list of strings\n",
    "    batch[\"teacher_outputs\"] = teacher_outputs\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/workspace/distillation_results\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"/workspace/logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=False,  # Disable FP16 since we're using BF16\n",
    "    bf16=bf16_supported,  # Use BF16 on A100 SXM\n",
    "    gradient_checkpointing=True,\n",
    "    no_cuda=False,\n",
    "    ddp_backend=None,\n",
    "    remove_unused_columns=False,  # Keep all columns, including teacher_outputs\n",
    ")\n",
    "\n",
    "# Initialize trainer with the custom data collator\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=custom_data_collator,  # Use the updated custom collator\n",
    ")\n",
    "\n",
    "# Train the student model, starting from scratch\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "# Aggregate training loss per epoch\n",
    "steps_per_epoch = len(train_dataset) // (batch_size * training_args.gradient_accumulation_steps)\n",
    "train_losses_per_epoch = [\n",
    "    np.mean(trainer.train_losses[i * steps_per_epoch: (i + 1) * steps_per_epoch])\n",
    "    for i in range(num_epochs)\n",
    "]\n",
    "\n",
    "# Compress checkpoints to save space\n",
    "import tarfile\n",
    "import glob\n",
    "checkpoint_dirs = glob.glob(\"/workspace/distillation_results/checkpoint-*\")\n",
    "for checkpoint_dir in checkpoint_dirs:\n",
    "    tar_path = f\"{checkpoint_dir}.tar.gz\"\n",
    "    with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "        tar.add(checkpoint_dir, arcname=os.path.basename(checkpoint_dir))\n",
    "    print(f\"Compressed checkpoint: {tar_path}\")\n",
    "\n",
    "print(\"\\nTraining completed.\")\n",
    "print(\"Training Loss per Epoch:\", [f\"{loss:.4f}\" for loss in train_losses_per_epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eec15ad-4df6-48d7-b085-8a65682d2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72016123-0b21-4d41-83dc-3f4b9c5358ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Prompt: Ryan was called by his friend to skip work one day.\n",
      "True Continuation: He missed his train to work and instead went to the park. Ryan and his friend played with birds at the park all day. At the end of the day, they left the park and saw Ryan's boss. Ryan got fired.\n",
      "Constraint Words: ['train', 'park', 'Ryan', 'friend', 'birds', 'day', 'end', 'boss']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:   5%|▌         | 1/20 [00:03<01:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Continuation: Ryan was called by his friend to skip work one day. He left the house at 8:30 AM, and it took him a certain amount of time before he got back home after having some food for lunch.\n",
      "The question is how long did Ryan spend in total on this trip?\n",
      "\n",
      "To solve problems like these where we are given information about an event that involves multiple steps (like traveling), let's break them down into their individual components.\n",
      "\n",
      "We have two key points here:\n",
      "\n",
      "- **Starting point**: The journey began when Ryans had already eaten breakfast\n",
      "Student Continuation: Ryan was called by his friend to skip work one day. He didn't take a break and went home, but the next morning he woke up at 3:00 PM with no sleep for about 10-15 minutes.\"\n",
      "\n",
      "Example 2:\n",
      "Prompt: Neil had been journeying through Asia.\n",
      "True Continuation: Now he had worked his way south into Australia. Neil was so excited to see Australian culture. He was thrilled at the prospect of exotic animals and people! His favorite moment was when he got to feed a baby koala bear.\n",
      "Constraint Words: ['way', 'Australia', 'Neil', 'culture', 'prospect', 'animals', 'people', 'moment', 'baby', 'bear']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  10%|█         | 2/20 [00:06<00:57,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Continuation: Neil had been journeying through Asia. He traveled for 3 days, then returned to the United States (US) and flew on a commercial aircraft where he was served by an airline that provided meals in plastic containers with their lids off? | Question: Why did Neil's meal container have its lid open?\n",
      "\n",
      "Choices:\n",
      "- To make sure it is not opened during flight\n",
      " - The food would be more appetizing if they were left unsealed.\n",
      "The correct answer:\n",
      "\n",
      "To find out why Neils'mealcontainerhaditslidopen\n",
      "Student Continuation: Neil had been journeying through Asia. He was in the middle of a long trip, and he decided to go back home for his family's Christmas Eve dinner with some friends who were staying at their parents' house.\"\n",
      "\"I'm sure I'll be very excited about it!\"\n",
      "\n",
      "Example 3:\n",
      "Prompt: My class went to the Everglades for our field trip.\n",
      "True Continuation: We did some sightseeing in several of the forests. We also got the opportunity to travel in water. The bus ride home was long and boring. I was tired when I got home.\n",
      "Constraint Words: ['forests', 'opportunity', 'water', 'bus', 'ride', 'home']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  15%|█▌        | 3/20 [00:09<00:52,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Continuation: My class went to the Everglades for our field trip. We saw a lot of wildlife, but I'm not sure if it's alligators or crocodiles? How can we tell them apart?\n",
      "\n",
      "What are some things that people do when they're in trouble and need help from others?\n",
      "Do you have any advice about how someone should handle an argument between two friends where one is being very aggressive with their words towards another person?\"\n",
      "\n",
      "Please answer these questions.\n",
      "Answer:\n",
      "The first question: To identify whether what was seen were aga... (I'll stop\n",
      "Student Continuation: My class went to the Everglades for our field trip. We had a lot of fun, and I'm very excited about it.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  20%|██        | 4/20 [00:12<00:49,  3.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  25%|██▌       | 5/20 [00:15<00:46,  3.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  30%|███       | 6/20 [00:19<00:45,  3.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  35%|███▌      | 7/20 [00:22<00:41,  3.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  40%|████      | 8/20 [00:25<00:38,  3.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  45%|████▌     | 9/20 [00:28<00:35,  3.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  50%|█████     | 10/20 [00:31<00:31,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  55%|█████▌    | 11/20 [00:34<00:27,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  60%|██████    | 12/20 [00:37<00:24,  3.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  65%|██████▌   | 13/20 [00:40<00:21,  3.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  70%|███████   | 14/20 [00:43<00:18,  3.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  75%|███████▌  | 15/20 [00:47<00:15,  3.16s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  80%|████████  | 16/20 [00:50<00:12,  3.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  85%|████████▌ | 17/20 [00:53<00:09,  3.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  90%|█████████ | 18/20 [00:56<00:06,  3.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference:  95%|█████████▌| 19/20 [00:59<00:03,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Inference: 100%|██████████| 20/20 [01:02<00:00,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference Results (At Least One Constraint Word Present):\n",
      "Teacher Constraint Word Inclusion Success Rate: 0.8500\n",
      "Student Constraint Word Inclusion Success Rate: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generate continuations and evaluate constraint word inclusion\n",
    "subset_eval_dataset = eval_dataset.select(range(min(inference_subset_size, len(eval_dataset))))\n",
    "teacher_success = 0\n",
    "student_success = 0\n",
    "\n",
    "teacher_model.eval()\n",
    "student_model.eval()\n",
    "\n",
    "for i, example in enumerate(tqdm(subset_eval_dataset, desc=\"Inference\")):\n",
    "    prompt = example[\"prompts\"]\n",
    "    constraint_words = example[\"constraint_words\"]  # Already a list, no eval() needed\n",
    "    true_continuation = example[\"continuations\"]\n",
    "    \n",
    "    # Debug: Print prompt, true continuation, and constraint words for the first few examples\n",
    "    if i < 3:  # Print for the first 3 examples\n",
    "        print(f\"\\nExample {i + 1}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"True Continuation: {true_continuation}\")\n",
    "        print(f\"Constraint Words: {constraint_words}\")\n",
    "    \n",
    "    teacher_tokenizer.padding_side = \"left\"\n",
    "    teacher_inputs = teacher_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length, return_attention_mask=True).to(device)\n",
    "    \n",
    "    student_tokenizer.padding_side = \"left\"\n",
    "    student_inputs = student_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length, return_attention_mask=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher_model.generate(\n",
    "            input_ids=teacher_inputs[\"input_ids\"],\n",
    "            attention_mask=teacher_inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,  # Increase to give more room for constraint words\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=1,  # Reduce to allow more flexibility (was 2)\n",
    "            early_stopping=True\n",
    "        )\n",
    "        teacher_continuation = teacher_tokenizer.decode(teacher_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        student_outputs = student_model.generate(\n",
    "            input_ids=student_inputs[\"input_ids\"],\n",
    "            attention_mask=student_inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,  # Increase to give more room for constraint words\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=1,  # Reduce to allow more flexibility (was 2)\n",
    "            early_stopping=True\n",
    "        )\n",
    "        student_continuation = student_tokenizer.decode(student_outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Debug: Print generated continuations for the first few examples\n",
    "    if i < 3:\n",
    "        print(f\"Teacher Continuation: {teacher_continuation}\")\n",
    "        print(f\"Student Continuation: {student_continuation}\")\n",
    "    \n",
    "    teacher_continuation_lower = teacher_continuation.lower()\n",
    "    student_continuation_lower = student_continuation.lower()\n",
    "    constraint_words_lower = [word.lower() for word in constraint_words]\n",
    "    \n",
    "    # Relaxed evaluation: Count as success if at least one constraint word is present\n",
    "    teacher_any_present = any(word in teacher_continuation_lower for word in constraint_words_lower)\n",
    "    student_any_present = any(word in student_continuation_lower for word in constraint_words_lower)\n",
    "    \n",
    "    if teacher_any_present:\n",
    "        teacher_success += 1\n",
    "    if student_any_present:\n",
    "        student_success += 1\n",
    "\n",
    "# Compute success rates\n",
    "teacher_success_rate = teacher_success / inference_subset_size\n",
    "student_success_rate = student_success / inference_subset_size\n",
    "\n",
    "print(\"\\nInference Results (At Least One Constraint Word Present):\")\n",
    "print(f\"Teacher Constraint Word Inclusion Success Rate: {teacher_success_rate:.4f}\")\n",
    "print(f\"Student Constraint Word Inclusion Success Rate: {student_success_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bebbe7f6-e647-4081-a1ba-0794843f1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plots Generated:\n",
      "- /workspace/loss_plot.png: Training Loss per Epoch\n",
      "- /workspace/constraint_success_plot.png: Teacher vs Student Constraint Word Inclusion Success Rate\n"
     ]
    }
   ],
   "source": [
    "# Plot 1: Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses_per_epoch, label=\"Training Loss\", color=\"blue\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"/workspace/loss_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Teacher vs Student Constraint Word Inclusion Success Rate\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar([\"Teacher (Qwen3-1.7B)\", \"Student (GPT-2)\"], [teacher_success_rate, student_success_rate], color=[\"orange\", \"purple\"])\n",
    "plt.ylabel(\"Success Rate\")\n",
    "plt.title(\"Teacher vs Student: Constraint Word Inclusion Success Rate\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis=\"y\")\n",
    "plt.savefig(\"/workspace/constraint_success_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nPlots Generated:\")\n",
    "print(\"- /workspace/loss_plot.png: Training Loss per Epoch\")\n",
    "print(\"- /workspace/constraint_success_plot.png: Teacher vs Student Constraint Word Inclusion Success Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80e219f7-ceee-42d4-a826-d3314be7478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed final model: /workspace/distilled_student_model.tar.gz\n",
      "\n",
      "Student model saved at: /workspace/distilled_student_model\n"
     ]
    }
   ],
   "source": [
    "# Save the student model to persistent storage\n",
    "student_model.save_pretrained(\"/workspace/distilled_student_model\")\n",
    "student_tokenizer.save_pretrained(\"/workspace/distilled_student_model\")\n",
    "\n",
    "# Compress the final model\n",
    "with tarfile.open(\"/workspace/distilled_student_model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"/workspace/distilled_student_model\", arcname=\"distilled_student_model\")\n",
    "print(\"Compressed final model: /workspace/distilled_student_model.tar.gz\")\n",
    "\n",
    "print(\"\\nStudent model saved at: /workspace/distilled_student_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94022eb3-7b63-4d99-b0c3-8d48e044505c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed huggingface_hub version: 0.33.0\n",
      "Repository here4code/distilled-gpt2-story-generation-Qwen3-1.7B already exists. Proceeding to upload to the existing repository...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a131cc50d45cfbd206f6e68bb1922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model folder uploaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py:9692: UserWarning: Warnings while validating metadata in README.md:\n",
      "- empty or missing yaml metadata in repo card\n",
      "  warnings.warn(f\"Warnings while validating metadata in README.md:\\n{message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model successfully pushed to: https://huggingface.co/here4code/distilled-gpt2-story-generation-Qwen3-1.7B\n",
      "Visit the link to view your model and enhance the model card if needed.\n"
     ]
    }
   ],
   "source": [
    "# Check the installed version of huggingface_hub\n",
    "import huggingface_hub\n",
    "print(f\"Installed huggingface_hub version: {huggingface_hub.__version__}\")\n",
    "\n",
    "from huggingface_hub import login, HfApi, create_repo, get_repo_discussions\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Define the repository ID\n",
    "repo_id = \"here4code/distilled-gpt2-story-generation-Qwen3-1.7B\"\n",
    "api = HfApi()\n",
    "\n",
    "# Step 3: Check if the repository already exists\n",
    "try:\n",
    "    get_repo_discussions(repo_id=repo_id, repo_type=\"model\")\n",
    "    repo_exists = True\n",
    "    print(f\"Repository {repo_id} already exists. Proceeding to upload to the existing repository...\")\n",
    "except Exception as e:\n",
    "    repo_exists = False\n",
    "    print(f\"Repository {repo_id} does not exist or cannot be accessed: {e}\")\n",
    "    print(\"Attempting to create the repository...\")\n",
    "\n",
    "# Step 4: Create the repository if it doesn't exist\n",
    "if not repo_exists:\n",
    "    try:\n",
    "        create_repo(repo_id=repo_id, repo_type=\"model\", private=False, exist_ok=False)\n",
    "        print(f\"Repository {repo_id} created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create repository: {e}\")\n",
    "        print(\"Possible issues:\")\n",
    "        print(\"- Token lacks write permissions. Generate a new token with write access at https://huggingface.co/settings/tokens.\")\n",
    "        print(\"- Namespace mismatch: Ensure the token is associated with the 'here4code' user.\")\n",
    "        print(\"- If the repository already exists, you may need to delete it or change the repo_id.\")\n",
    "        raise\n",
    "\n",
    "# Step 5: Upload the model folder\n",
    "try:\n",
    "    api.upload_folder(\n",
    "        folder_path=\"/workspace/distilled_student_model\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload distilled GPT-2 story generation model\"\n",
    "    )\n",
    "    print(\"Model folder uploaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during upload: {e}\")\n",
    "    print(\"Ensure the token has write permissions and the repository is accessible.\")\n",
    "    raise\n",
    "\n",
    "# Step 6: Create and upload a basic model card (README.md)\n",
    "model_card_content = \"\"\"\n",
    "# Distilled GPT-2 Story Generation Model (June 2025)\n",
    "\n",
    "This is a distilled version of GPT-2, fine-tuned using knowledge distillation from a teacher model (Qwen3-1.7B) on the ROCStories dataset. The model is designed for story generation with constraint words.\n",
    "\n",
    "## Model Details\n",
    "- **Base Model**: GPT-2\n",
    "- **Teacher Model**: Qwen3-1.7B\n",
    "- **Dataset**: ROCStories (Ximing/ROCStories)\n",
    "- **Training Objective**: Knowledge distillation to match teacher outputs.\n",
    "- **Training Date**: June 12, 2025\n",
    "- **Evaluation**: Constraint word inclusion success rate.\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"{repo_id}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Once upon a time, there was a happy dog\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "- **Epochs**: {num_epochs}\n",
    "- **Batch Size**: {batch_size}\n",
    "- **Training Loss per Epoch**: {train_losses_per_epoch}\n",
    "\n",
    "## Evaluation Results\n",
    "- **Teacher Constraint Word Inclusion Success Rate**: {teacher_success_rate:.4f}\n",
    "- **Student Constraint Word Inclusion Success Rate**: {student_success_rate:.4f}\n",
    "\n",
    "## License\n",
    "This model is released under the MIT License.\n",
    "\"\"\"\n",
    "\n",
    "# Write the model card to a README.md file and upload it\n",
    "with open(\"/workspace/distilled_student_model/README.md\", \"w\") as f:\n",
    "    f.write(model_card_content.format(\n",
    "        repo_id=repo_id,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        train_losses_per_epoch=train_losses_per_epoch,\n",
    "        teacher_success_rate=teacher_success_rate,\n",
    "        student_success_rate=student_success_rate\n",
    "    ))\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/workspace/distilled_student_model/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    commit_message=\"Add model card\"\n",
    ")\n",
    "\n",
    "print(f\"\\nModel successfully pushed to: https://huggingface.co/{repo_id}\")\n",
    "print(f\"Visit the link to view your model and enhance the model card if needed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d52ab-d0c7-4ad6-8b18-7a53ca0bb610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
